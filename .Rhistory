train<- house_train
test<- house_test
for (j in 1:length(method_meta)){
# j <- 1
# if(method_meta[j] %in% c("xgbTree", "rf")){
#
#   designMat <- model.matrix(lm(house_train$housePrice~.,data=house_train))
#   designMat <- designMat[,-1]
#
# } else if (method_meta[j] %in% c("gbm", "knn")){
#
#   designMat <- house_train[,-1]
#
# }
modelfit <- train(housePrice~.,
data  = house_train,
method = method_meta[j],
preProc = c("center","scale"),
trControl = trainControl(method="none"),
tuneGrid = parametersTuned_meta[[j]])
metaPred1[,j] <- predict(modelfit, newdata = house_test)
metaPred2[,j] <- predict(modelfit, newdata = house_train)
metaPred11[,j] <- predict(modelfit, newdata = house_train)
metaPred21[,j] <- predict(modelfit, newdata = house_train)
}
metaPred1
emfit1 <- lm(train$housePrice ~ metaPred2)
emfit1
emfit2 <- train(y = train$housePrice,
x = as.data.frame(metaPred2),
tuneGrid = data.frame(mtry=1:50),
method = "rf", ntree = 150,
trControl = trainControl(method="oob"))
emfit2
plot(emfit2)
emfit11 <- lm(train$housePrice ~ metaPred21)
folds_meta
rm(folds_meta)
folds
rm(folds)
rm(shuffle_meta)
rm(shuffle)
train_meta
rm(train_meta)
View(an)
rm(designMat)
rm(designMatENET)
rm(designMatPCR)
rm(designMatPLS)
rm(designMatRF)
View(ENET_prediction_plot)
View(enetPrediction)
NNET_prediction_plot
View(ENetTune_housePrice)
View(ENET_prediction_plot)
View(LM_prediction_plot)
KNN_prediction_plot
rm(KNN_prediction_plot)
rm(NNET_prediction_plot)
LM_prediction_plot
rm(LM_prediction_plot)
RF_prediction_plot
rm(RF_prediction_plot)
TREE_prediction_plot
rm(TREE_prediction_plot)
GBM_prediction_plot
rm(GBM_prediction_plot)
XGB_prediction_plot
rm(XGB_prediction_plot)
ENET_prediction_plot
rm(ENET_prediction_plot)
PCR_prediction_plot
rm(PCR_prediction_plot)
PLS_prediction_plot
rm(PLS_prediction_plot)
lapply(packagesToLoad, require, character.only = TRUE)
metaStack1
plot(emfit21)
plot(emfit2)
emfit11 <- lm(train$housePrice ~ metaPred21)
emfit21 <- train(y = train$housePrice,
x = as.data.frame(metaPred21),
tuneGrid = data.frame(mtry=1:50),
method = "rf", ntree = 150,
trControl = trainControl(method="oob"))
fittedModelPlot
fit.model
treeBestTune
fit.model <- rpart(house_train[,1]~., data = house_train, cp = treeBestTune)
lapply(packagesToLoad, require, character.only = TRUE)
### Here instead of loading each package seperately, I load all of them at the same time.
packagesToLoad <- c("stringr", "Matrix", "glmnet", "xgboost", "randomForest", "Metrics", "caret", "scales",
"e1071", "corrplot", "psych", "tidyverse", "lubridate", "pls", "gdata", "graphics", "rpart",
"gbm", "earth", "Boruta", "ggcorrplot", "rpart.plot")
lapply(packagesToLoad, require, character.only = TRUE)
fit.model <- rpart(house_train[,1]~., data = house_train, cp = treeBestTune)
fittedModelPlot <- rpart.plot(fit.model, main = "Fitted Model")
plot(xgbTune_housePrice)
xgbTune_housePrice
plot(xgbTune_housePrice)
xgbresults
devtools::install_github("hadley/emo")
require(emo)
designMatENET <- model.matrix(lm(housePrice~.,data=house_train))
designMatENET <- designMat[,-1]
Enet_grid <- expand.grid(alpha = seq(0,.5,length.out=15),
lambda = seq(10,500,length.out=15))
ENetTune_housePrice <- train(y = house_train[,1], x = designMatENET,
method = "glmnet",
preProcess = c("center","scale"),
tuneGrid = Enet_grid,
trControl = trainControl(method="repeatedcv", repeats = 2, number = 10))
ENetTune_housePrice <- train(y = house_train[,1], x = designMatENET,
method = "glmnet",
tuneGrid = Enet_grid,
trControl = trainControl(method="repeatedcv", repeats = 2, number = 10))
pcrBestTune
plot(pcrTune_housePrice)
nnetPrediction
nnetPrediction <- cbind(train_housePrice, NNET_housePrice)%>%as.data.frame()
nnetPrediction <- nnetPrediction/1000000
NNET_prediction_plot <- ggplot(data = nnetPrediction, aes(x = train_housePrice,
y = NNET_housePrice)) + geom_jitter() + geom_smooth(method = loess) +
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="NNet Predicted House Price (in Million $)")
NNET_prediction_plot
knnPrediction <- knnPrediction/1000000
KNN_prediction_plot <- ggplot(data = knnPrediction, aes(x = train_housePrice,
y = KNN_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="kNN Predicted House Price (in Million $)")
KNN_prediction_plot
lmPrediction <- lmPrediction/1000000
LM_prediction_plot <- ggplot(data = lmPrediction, aes(x = train_housePrice,
y = LM_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="MLR Predicted House Price (in Million $)")
LM_prediction_plot
rfPrediction <- rfPrediction/1000000
RF_prediction_plot <- ggplot(data = rfPrediction, aes(x = train_housePrice,
y = RF_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="RF Predicted House Price (in Million $)")
RF_prediction_plot
TREE_prediction_plot <- ggplot(data = treePrediction, aes(x = train_housePrice,
y = TREE_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Decision Tree Predicted House Price (in Million $)")
treePrediction <- treePrediction/1000000
TREE_prediction_plot <- ggplot(data = treePrediction, aes(x = train_housePrice,
y = TREE_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Decision Tree Predicted House Price (in Million $)")
TREE_prediction_plot
gbmPrediction <- gbmPrediction/1000000
GBM_prediction_plot <- ggplot(data = gbmPrediction, aes(x = train_housePrice,
y = GBM_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="GBM Predicted House Price (in Million $)")
GBM_prediction_plot
xgbPrediction <- xgbPrediction/1000000
XGB_prediction_plot <- ggplot(data = xgbPrediction, aes(x = train_housePrice,
y = XGB_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="XGB Predicted House Price (in Million $)")
XGB_prediction_plot
enetPrediction <- enetPrediction/1000000
ENET_prediction_plot <- ggplot(data = enetPrediction, aes(x = train_housePrice,
y = ENET_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="ENET Predicted House Price (in Million $)")
ENET_prediction_plot
pcrPrediction <- pcrPrediction/1000000
PCR_prediction_plot <- ggplot(data = pcrPrediction, aes(x = train_housePrice,
y = PCR_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="PCR Predicted House Price (in Million $)")
PCR_prediction_plot
plsPrediction <- plsPrediction/1000000
PLS_prediction_plot <- ggplot(data = plsPrediction, aes(x = train_housePrice,
y = PLS_housePrice)) + geom_jitter() + geom_smooth(method = loess)+
scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="PLS Predicted House Price (in Million $)")
PLS_prediction_plot
KNN_prediction_plot_stack
KNN_housePriceStack
### Here I arrange the predicted price from stacking using kNN (note this contains the methods
### "xgbTree", "rf", "gbm", "knn", stacking of those methods with OLS and a stack of all methods OLS and RF)
### So it is safe to call this super super stack
KNN_housePriceStack <- metaStack1$kNN
emfit1
metaStack
metaPred1
emfit2_kNN
emfit21
emfit2
plot(emfit2)
plot(emfit21)
emfit11
emfit21
plot(emfit2)
plot(emfit2)
emfit2$bestTune
emfit11
emfit1
metaStack1
metaPred1
emfit2_kNN
nnet.gridMeta <- expand.grid(size = seq(from = 1, to = 6, length.out = 6),
decay = seq(from = .3, to = .8, length.out = 6))
emfit2_kNN <- train(y = train$housePrice,
x = as.data.frame(metaPred21),
method = "knn",
preProcess = c("center","scale"),
tuneGrid = data.frame(.k=1:20),
trControl = trainControl(method = "repeatedcv", repeats = 3, number = 10))
### Here instead of loading each package seperately, I load all of them at the same time.
packagesToLoad <- c("stringr", "Matrix", "glmnet", "xgboost", "randomForest", "Metrics", "caret", "scales",
"e1071", "corrplot", "psych", "tidyverse", "lubridate", "pls", "gdata", "graphics", "rpart",
"gbm", "earth", "Boruta", "ggcorrplot", "rpart.plot")
lapply(packagesToLoad, require, character.only = TRUE)
plot(emfit2)
nnet.gridMeta <- expand.grid(size = seq(from = 1, to = 6, length.out = 6),
decay = seq(from = .3, to = .8, length.out = 6))
emfit2_kNN <- train(y = train$housePrice,
x = as.data.frame(metaPred21),
method = "knn",
preProcess = c("center","scale"),
tuneGrid = data.frame(.k=1:20),
trControl = trainControl(method = "repeatedcv", repeats = 3, number = 10))
metaPred1<- matrix( NA, nrow = dim(house_test)[1], ncol = length(method_meta) + 3)
metaPred2<- matrix( NA, nrow = dim(house_train)[1], ncol = length(method_meta))
metaPred11<- matrix( NA, nrow = dim(house_train)[1], ncol = length(method_meta) + 3)
metaPred21<- matrix( NA, nrow = dim(house_train)[1], ncol = length(method_meta))
train <- house_train
test <- house_test
for (j in 1:length(method_meta)){
# j <- 1
# if(method_meta[j] %in% c("xgbTree", "rf")){
#
#   designMat <- model.matrix(lm(house_train$housePrice~.,data=house_train))
#   designMat <- designMat[,-1]
#
# } else if (method_meta[j] %in% c("gbm", "knn")){
#
#   designMat <- house_train[,-1]
#
# }
modelfit <- train(housePrice~.,
data  = house_train,
method = method_meta[j],
preProc = c("center","scale"),
trControl = trainControl(method="none"),
tuneGrid = parametersTuned_meta[[j]])
metaPred1[,j] <- predict(modelfit, newdata = house_test)
metaPred2[,j] <- predict(modelfit, newdata = house_train)
metaPred11[,j] <- predict(modelfit, newdata = house_train)
metaPred21[,j] <- predict(modelfit, newdata = house_train)
}
metaPred1<- matrix( NA, nrow = dim(house_test)[1], ncol = length(method_meta) + 2)
metaPred2<- matrix( NA, nrow = dim(house_train)[1], ncol = length(method_meta))
metaPred11<- matrix( NA, nrow = dim(house_train)[1], ncol = length(method_meta) + 2)
metaPred21<- matrix( NA, nrow = dim(house_train)[1], ncol = length(method_meta))
train <- house_train
test <- house_test
for (j in 1:length(method_meta)){
# j <- 1
# if(method_meta[j] %in% c("xgbTree", "rf")){
#
#   designMat <- model.matrix(lm(house_train$housePrice~.,data=house_train))
#   designMat <- designMat[,-1]
#
# } else if (method_meta[j] %in% c("gbm", "knn")){
#
#   designMat <- house_train[,-1]
#
# }
modelfit <- train(housePrice~.,
data  = house_train,
method = method_meta[j],
preProc = c("center","scale"),
trControl = trainControl(method="none"),
tuneGrid = parametersTuned_meta[[j]])
metaPred1[,j] <- predict(modelfit, newdata = house_test)
metaPred2[,j] <- predict(modelfit, newdata = house_train)
metaPred11[,j] <- predict(modelfit, newdata = house_train)
metaPred21[,j] <- predict(modelfit, newdata = house_train)
}
emfit1 <- lm(train$housePrice ~ metaPred2)
emfit2 <- train(y = train$housePrice,
x = as.data.frame(metaPred2),
tuneGrid = data.frame(mtry=1:50),
method = "rf", ntree = 150,
trControl = trainControl(method="oob"))
plot(emfit2)
emfit2$bestTune
metaPred2
plot(emfit2)
metaPred21
metaPred21 == metaPred2
nnet.gridMeta <- expand.grid(size = seq(from = 1, to = 6, length.out = 6),
decay = seq(from = .3, to = .8, length.out = 6))
emfit2_kNN <- train(y = train$housePrice,
x = as.data.frame(metaPred21),
method = "knn",
preProcess = c("center","scale"),
tuneGrid = data.frame(.k=1:20),
trControl = trainControl(method = "repeatedcv", repeats = 3, number = 10))
plot(emfit2_kNN)
emfit2_kNN$bestTune
emfit2
emfit1
emfit11
metaPred21
metaPred21 == metaPred11
emfit21
emfit11
metaPred2
metaPred2 == metaPred21
metaPred1[,5] <- cbind( matrix(1,nrow=dim(house_test)[1],ncol=1) , metaPred1[,-c(5,6)] ) %*% coef(emfit1)
metaPred1[,6] <- predict( emfit2, as.data.frame(metaPred1))
metaPred11[,5] <- cbind( matrix(1,nrow=dim(house_train)[1],ncol=1) , metaPred11[,-c(5,6)] ) %*% coef(emfit11)
metaPred11[,6] <- predict( emfit21, as.data.frame(metaPred11))
metaPred11[,5] <- cbind( matrix(1,nrow=dim(house_train)[1],ncol=1) , metaPred11[,-c(5,6)] ) %*% coef(emfit1)
metaPred11[,6] <- predict( emfit2, as.data.frame(metaPred11))
metaStack<- as.data.frame(metaPred1)
colnames(metaStack)[1:4]<- method_meta
colnames(metaStack)[5:6]<- c("enOLS","enFOR")
metaStack1<- as.data.frame(metaPred11)
colnames(metaStack1)[1:4]<- method_meta
colnames(metaStack1)[5:6]<- c("enOLS","enFOR")
metaStack1$kNN <- predict( emfit2_kNN, as.data.frame(metaPred11))
metaStack$kNN <- predict( emfit2_kNN, as.data.frame(metaPred1))
metaStack
metaStack1
### Here I arrange the predicted price from stacking using random forest (note this contains the methods
### "xgbTree", "rf", "gbm", "knn", stacking of those methods with OLS and a stack of all methods and OLS)
### So it is safe to call this super stack
RF_housePriceStack <- metaStack1$enFOR
RF_housePriceStack <- cbind(train_housePrice,RF_housePriceStack)%>%as.data.frame()
RF_housePriceStack <- RF_housePriceStack/1000000
RF_prediction_plot_stack <- ggplot(data=RF_housePriceStack,aes(x=train_housePrice,
y=RF_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs Random Forest Super Stacked Predicted House Price")
#### Here I arrange the predicted price from stacking using OLS
OLS_housePriceStack<- metaStack1$enOLS
OLS_housePriceStack <- cbind(train_housePrice,OLS_housePriceStack)%>%as.data.frame()
OLS_housePriceStack <- OLS_housePriceStack/1000000
OLS_prediction_plot_stack <- ggplot(data=OLS_housePriceStack,aes(x=train_housePrice,
y=OLS_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs OLS Stacked Predicted House Price")
### Here I arrange the predicted price from stacking using kNN (note this contains the methods
### "xgbTree", "rf", "gbm", "knn", stacking of those methods with OLS and a stack of all methods OLS and RF)
### So it is safe to call this super super stack
KNN_housePriceStack <- metaStack1$kNN
KNN_housePriceStack <- cbind(train_housePrice,KNN_housePriceStack)%>%as.data.frame()
KNN_housePriceStack <- KNN_housePriceStack/1000000
KNN_prediction_plot_stack <- ggplot(data=KNN_housePriceStack,aes(x=train_housePrice,
y=KNN_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs KNN Super Super Stacked Predicted House Price")
RF_prediction_plot_stack
### Here I arrange the predicted price from stacking using random forest (note this contains the methods
### "xgbTree", "rf", "gbm", "knn", stacking of those methods with OLS and a stack of all methods and OLS)
### So it is safe to call this super stack
RF_housePriceStack <- metaStack1$enFOR
RF_housePriceStack <- cbind(train_housePrice,RF_housePriceStack)%>%as.data.frame()
RF_housePriceStack <- RF_housePriceStack/1000000
RF_prediction_plot_stack <- ggplot(data=RF_housePriceStack,aes(x=train_housePrice,
y=RF_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs Random Forest Stacked Predicted House Price")
#### Here I arrange the predicted price from stacking using OLS
OLS_housePriceStack<- metaStack1$enOLS
OLS_housePriceStack <- cbind(train_housePrice,OLS_housePriceStack)%>%as.data.frame()
OLS_housePriceStack <- OLS_housePriceStack/1000000
OLS_prediction_plot_stack <- ggplot(data=OLS_housePriceStack,aes(x=train_housePrice,
y=OLS_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs OLS Stacked Predicted House Price")
### Here I arrange the predicted price from stacking using kNN (note this contains the methods
### "xgbTree", "rf", "gbm", "knn", stacking of those methods with OLS and a stack of all methods OLS and RF)
### So it is safe to call this super super stack
KNN_housePriceStack <- metaStack1$kNN
KNN_housePriceStack <- cbind(train_housePrice,KNN_housePriceStack)%>%as.data.frame()
KNN_housePriceStack <- KNN_housePriceStack/1000000
KNN_prediction_plot_stack <- ggplot(data=KNN_housePriceStack,aes(x=train_housePrice,
y=KNN_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs KNN Stacked Predicted House Price")
OLS_prediction_plot_stack
RF_prediction_plot_stack
KNN_prediction_plot_stack
emfit2_RF <- emfit2
plot(emfit2_RF)
### Training
metaPred11[,5] <- cbind( matrix(1,nrow=dim(house_train)[1],ncol=1) , metaPred11[,-c(5,6)] ) %*% coef(emfit1)
metaPred11[,6] <- predict( emfit2, as.data.frame(metaPred11))
metaPred11[,6] <- predict( emfit2_kNN, as.data.frame(metaPred11))
metaStack1<- as.data.frame(metaPred11)
colnames(metaStack1)[1:4]<- method_meta
colnames(metaStack1)[5:6]<- c("enOLS","enKNN")
metaStack1$enRF <- predict( emfit2_RF, as.data.frame(metaPred11))
metaStack1
### Here I arrange the predicted price from stacking using random forest (note this contains the methods
### "xgbTree", "rf", "gbm", "knn", stacking of those methods with OLS and a stack of all methods and OLS)
### So it is safe to call this super stack
RF_housePriceStack <- metaStack1$enFOR
RF_housePriceStack <- cbind(train_housePrice,RF_housePriceStack)%>%as.data.frame()
RF_housePriceStack <- RF_housePriceStack/1000000
RF_prediction_plot_stack <- ggplot(data=RF_housePriceStack,aes(x=train_housePrice,
y=RF_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs Random Forest Stacked Predicted House Price")
RF_prediction_plot_stack
RF_housePriceStack
metaStack1$enFOR
emfit1
metaPred11
cbind( matrix(1,nrow=dim(house_train)[1],ncol=1) , metaPred11[,-c(5,6)] )
cbind( matrix(1,nrow=dim(house_train)[1],ncol=1) , metaPred11[,-c(5,6)] ) %*% coef(emfit1)
### Training
metaPred11[,5] <- cbind( matrix(1,nrow=dim(house_train)[1],ncol=1) , metaPred11[,-c(5,6)] ) %*% coef(emfit1)
metaPred11[,6] <- predict( emfit2_kNN, as.data.frame(metaPred11))
metaStack1<- as.data.frame(metaPred11)
colnames(metaStack1)[1:4]<- method_meta
colnames(metaStack1)[5:6]<- c("enOLS","enKNN")
metaStack1
predict( emfit2_RF, as.data.frame(metaStack1))
predict( emfit2_RF, as.data.frame(metaPred11))
metaStack1$enRF <- predict( emfit2_RF, as.data.frame(metaPred11))
metaStack1
### Here I arrange the predicted price from stacking using random forest (note this contains the methods
### "xgbTree", "rf", "gbm", "knn", stacking of those methods with OLS and a stack of all methods and OLS)
### So it is safe to call this super stack
RF_housePriceStack <- metaStack1$enRF
RF_housePriceStack <- cbind(train_housePrice,RF_housePriceStack)%>%as.data.frame()
RF_housePriceStack <- RF_housePriceStack/1000000
RF_prediction_plot_stack <- ggplot(data=RF_housePriceStack,aes(x=train_housePrice,
y=RF_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs Random Forest Stacked Predicted House Price")
RF_prediction_plot_stack
#### Here I arrange the predicted price from stacking using OLS
OLS_housePriceStack<- metaStack1$enOLS
OLS_housePriceStack <- cbind(train_housePrice,OLS_housePriceStack)%>%as.data.frame()
OLS_housePriceStack <- OLS_housePriceStack/1000000
OLS_prediction_plot_stack <- ggplot(data=OLS_housePriceStack,aes(x=train_housePrice,
y=OLS_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs OLS Stacked Predicted House Price")
OLS_prediction_plot_stack
### Here I arrange the predicted price from stacking using kNN (note this contains the methods
### "xgbTree", "rf", "gbm", "knn", stacking of those methods with OLS and a stack of all methods OLS and RF)
### So it is safe to call this super super stack
KNN_housePriceStack <- metaStack1$enKNN
KNN_housePriceStack <- cbind(train_housePrice,KNN_housePriceStack)%>%as.data.frame()
KNN_housePriceStack <- KNN_housePriceStack/1000000
KNN_prediction_plot_stack <- ggplot(data=KNN_housePriceStack,aes(x=train_housePrice,
y=KNN_housePriceStack))+
geom_jitter()+geom_smooth(method = loess) + scale_x_continuous(name = "Train House Price (in Million $)") +
scale_y_continuous(name="Predicted House Price (in Million $)") + ggtitle("Train vs KNN Stacked Predicted House Price")
KNN_prediction_plot_stack
KNN_prediction_plot_stack
RF_prediction_plot_stack
metaStack
##### Testing
metaPred1[,5] <- cbind( matrix(1,nrow=dim(house_test)[1],ncol=1) , metaPred1[,-c(5,6)] ) %*% coef(emfit1)
metaPred1[,6] <- predict( emfit2_kNN, as.data.frame(metaPred1))
metaStack<- as.data.frame(metaPred1)
colnames(metaStack)[1:4]<- method_meta
colnames(metaStack)[5:6]<- c("enOLS","enKNN")
metaStack$enRF <- predict( emfit2_RF, as.data.frame(metaPred1))
metaStack
metaPred1[,5] <- cbind( matrix(1,nrow=dim(house_test)[1],ncol=1) , metaPred1[,-c(5,6)] ) %*% coef(emfit1)
metaPred1[,6] <- predict( emfit2_kNN, as.data.frame(metaPred1))
metaStack<- as.data.frame(metaPred1)
colnames(metaStack)[1:4]<- method_meta
colnames(metaStack)[5:6]<- c("enOLS","enKNN")
metaStack$kNN <- predict( emfit2_RF, as.data.frame(metaPred1))
metaStack %>% head()
metaStack
cbind(metaStack$xgbTree, house_test)
metaStack$xgbTree
metaStack %>% head()
metaStack
##### Testing
metaPred1[,5] <- cbind( matrix(1,nrow=dim(house_test)[1],ncol=1) , metaPred1[,-c(5,6)] ) %*% coef(emfit1)
metaPred1[,6] <- predict( emfit2_kNN, as.data.frame(metaPred1))
metaStack<- as.data.frame(metaPred1)
colnames(metaStack)[1:4]<- method_meta
colnames(metaStack)[5:6]<- c("enOLS","enKNN")
metaStack$enRF <- predict( emfit2_RF, as.data.frame(metaPred1))
metaStack
metaStack %>% head()
names(metaStack) <- c("priceXGB", "priceRF", "priceGBM",
"priceKNN", "priceEnOLS", "priceEnKNN", "priceEnRF")
houseTestPrice <- cbind(metaStack, house_test)
houseTestPrice
houseTestPrice <- cbind(metaStack, house_test)
testPredictedHousePrice <- cbind(metaStack, house_test)
testPredictedHousePriceXGB <- testPredictedHousePrice %>% select(-priceRF, - priceGBM, -priceKNN,
-priceEnOLS, -priceEnKNN, -priceEnRF)
testPredictedHousePriceXGB
names(metaStack) <- c("priceXGB", "priceRF", "priceGBM",
"priceKNN", "priceEnOLS", "priceEnKNN", "priceEnRF")
testPredictedHousePrice <- cbind(metaStack, house_test)
write.csv(testPredictedHousePrice,file="testPredicted/testPredictedHousePrice.csv")
write.csv(testPredictedHousePrice,file="testPredictedHousePrice.csv")
housePrice
